<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>TransferEntropy.jl ¬∑ TransferEntropy.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">TransferEntropy.jl</span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>TransferEntropy.jl</a><ul class="internal"><li><a class="tocitem" href="#Estimators"><span>Estimators</span></a></li><li><a class="tocitem" href="#Transfer-entropy"><span>Transfer entropy</span></a></li><li><a class="tocitem" href="#Mutual-information"><span>Mutual information</span></a></li><li><a class="tocitem" href="#Dataset"><span>Dataset</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>TransferEntropy.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>TransferEntropy.jl</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/TransferEntropy.jl/blob/master/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="TransferEntropy.jl"><a class="docs-heading-anchor" href="#TransferEntropy.jl">TransferEntropy.jl</a><a id="TransferEntropy.jl-1"></a><a class="docs-heading-anchor-permalink" href="#TransferEntropy.jl" title="Permalink"></a></h1><p>This package exports two functions, <a href="#TransferEntropy.transferentropy"><code>transferentropy</code></a> and <a href="#TransferEntropy.mutualinfo"><code>mutualinfo</code></a>. Both functions use the estimators listed below (not all estimators are implemented for both  functions; see docstrings for <a href="#TransferEntropy.transferentropy"><code>transferentropy</code></a> and <a href="#TransferEntropy.mutualinfo"><code>mutualinfo</code></a> for  details).</p><h2 id="Estimators"><a class="docs-heading-anchor" href="#Estimators">Estimators</a><a id="Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Estimators" title="Permalink"></a></h2><h3 id="Binning-based"><a class="docs-heading-anchor" href="#Binning-based">Binning based</a><a id="Binning-based-1"></a><a class="docs-heading-anchor-permalink" href="#Binning-based" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.VisitationFrequency" href="#Entropies.VisitationFrequency"><code>Entropies.VisitationFrequency</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">VisitationFrequency(r::RectangularBinning) &lt;: BinningProbabilitiesEstimator</code></pre><p>A probability estimator based on binning data into rectangular boxes dictated by the binning scheme <code>r</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia"># Construct boxes by dividing each coordinate axis into 5 equal-length chunks.
b = RectangularBinning(5)

# A probabilities estimator that, when applied a dataset, computes visitation frequencies
# over the boxes of the binning, constructed as describedon the previous line.
est = VisitationFrequency(b)</code></pre><p>See also: <a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.RectangularBinning" href="#Entropies.RectangularBinning"><code>Entropies.RectangularBinning</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RectangularBinning(œµ) &lt;: RectangularBinningScheme</code></pre><p>Instructions for creating a rectangular box partition using the binning scheme <code>œµ</code>.  Binning instructions are deduced from the type of <code>œµ</code>.</p><p>Rectangular binnings may be automatically adjusted to the data in which the <code>RectangularBinning</code>  is applied, as follows:</p><ol><li><p><code>œµ::Int</code> divides each coordinate axis into <code>œµ</code> equal-length intervals,   extending the upper bound 1/100th of a bin size to ensure all points are covered.</p></li><li><p><code>œµ::Float64</code> divides each coordinate axis into intervals of fixed size <code>œµ</code>, starting   from the axis minima until the data is completely covered by boxes.</p></li><li><p><code>œµ::Vector{Int}</code> divides the i-th coordinate axis into <code>œµ[i]</code> equal-length   intervals, extending the upper bound 1/100th of a bin size to ensure all points are   covered.</p></li><li><p><code>œµ::Vector{Float64}</code> divides the i-th coordinate axis into intervals of fixed size <code>œµ[i]</code>, starting   from the axis minima until the data is completely covered by boxes.</p></li></ol><p>Rectangular binnings may also be specified on arbitrary min-max ranges. </p><ol><li><code>œµ::Tuple{Vector{Tuple{Float64,Float64}},Int64}</code> creates intervals   along each coordinate axis from ranges indicated by a vector of <code>(min, max)</code> tuples, then divides   each coordinate axis into an integer number of equal-length intervals. <em>Note: this does not ensure   that all points are covered by the data (points outside the binning are ignored)</em>.</li></ol><p><strong>Example 1: Grid deduced automatically from data (partition guaranteed to cover data points)</strong></p><p><strong>Flexible box sizes</strong></p><p>The following binning specification finds the minima/maxima along each coordinate axis, then  split each of those data ranges (with some tiny padding on the edges) into <code>10</code> equal-length  intervals. This gives (hyper-)rectangular boxes, and works for data of any dimension.</p><pre><code class="language-julia">using Entropies
RectangularBinning(10)</code></pre><p>Now, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.</p><p>The following binning specification finds the minima/maxima along each coordinate axis, then  splits the range along the first coordinate axis (with some tiny padding on the edges)  into <code>10</code> equal-length intervals, and the range along the second coordinate axis (with some  tiny padding on the edges) into <code>5</code> equal-length intervals. This gives (hyper-)rectangular boxes.</p><pre><code class="language-julia">using Entropies
RectangularBinning([10, 5])</code></pre><p><strong>Fixed box sizes</strong></p><p>The following binning specification finds the minima/maxima along each coordinate axis,  then split the axis ranges into equal-length intervals of fixed size <code>0.5</code> until the all data  points are covered by boxes. This approach yields (hyper-)cubic boxes, and works for  data of any dimension.</p><pre><code class="language-julia">using Entropies
RectangularBinning(0.5)</code></pre><p>Again, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.</p><p>The following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis into equal-length intervals of size <code>0.3</code>, and the range along the second axis into equal-length intervals of size <code>0.1</code> (in both cases,  making sure the data are completely covered by the boxes). This approach gives a (hyper-)rectangular boxes. </p><pre><code class="language-julia">using Entropies
RectangularBinning([0.3, 0.1])</code></pre><p><strong>Example 2: Custom grids (partition not guaranteed to cover data points):</strong></p><p>Assume the data consists of 3-dimensional points <code>(x, y, z)</code>, and that we want a grid  that is fixed over the intervals <code>[x‚ÇÅ, x‚ÇÇ]</code> for the first dimension, over <code>[y‚ÇÅ, y‚ÇÇ]</code> for the second dimension, and over <code>[z‚ÇÅ, z‚ÇÇ]</code> for the third dimension. We when want to split each of those ranges into 4 equal-length pieces. <em>Beware: some points may fall  outside the partition if the intervals are not chosen properly (these points are  simply discarded)</em>. </p><p>The following binning specification produces the desired (hyper-)rectangular boxes. </p><pre><code class="language-julia">using Entropies, DelayEmbeddings

D = Dataset(rand(100, 3));

x‚ÇÅ, x‚ÇÇ = 0.5, 1 # not completely covering the data, which are on [0, 1]
y‚ÇÅ, y‚ÇÇ = -2, 1.5 # covering the data, which are on [0, 1]
z‚ÇÅ, z‚ÇÇ = 0, 0.5 # not completely covering the data, which are on [0, 1]

œµ = [(x‚ÇÅ, x‚ÇÇ), (y‚ÇÅ, y‚ÇÇ), (z‚ÇÅ, z‚ÇÇ)], 4 # [interval 1, interval 2, ...], n_subdivisions

RectangularBinning(œµ)</code></pre></div></section></article><h3 id="Kernel-density-based"><a class="docs-heading-anchor" href="#Kernel-density-based">Kernel density based</a><a id="Kernel-density-based-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-density-based" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.NaiveKernel" href="#Entropies.NaiveKernel"><code>Entropies.NaiveKernel</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NaiveKernel(œµ::Real, method::KernelEstimationMethod = TreeDistance()) &lt;: ProbabilitiesEstimator</code></pre><p>Estimate probabilities/entropy using a &quot;naive&quot; kernel density estimation approach (KDE), as  discussed in Prichard and Theiler (1995) <sup class="footnote-reference"><a id="citeref-PrichardTheiler1995" href="#footnote-PrichardTheiler1995">[PrichardTheiler1995]</a></sup>.</p><p>Probabilities <span>$P(\mathbf{x}, \epsilon)$</span> are assigned to every point <span>$\mathbf{x}$</span> by  counting how many other points occupy the space spanned by  a hypersphere of radius <code>œµ</code> around <span>$\mathbf{x}$</span>, according to:</p><p class="math-container">\[P_i( \mathbf{x}, \epsilon) \approx \dfrac{1}{N} \sum_{s \neq i } K\left( \dfrac{||\mathbf{x}_i - \mathbf{x}_s ||}{\epsilon} \right),\]</p><p>where <span>$K(z) = 1$</span> if <span>$z &lt; 1$</span> and zero otherwise. Probabilities are then normalized.</p><p><strong>Methods</strong></p><ul><li>Tree-based evaluation of distances using <a href="#Entropies.TreeDistance"><code>TreeDistance</code></a>. Faster, but more   memory allocation.</li><li>Direct evaluation of distances using <a href="#Entropies.DirectDistance"><code>DirectDistance</code></a>. Slower, but less    memory allocation. Also works for complex numbers.</li></ul><p><strong>Estimation</strong></p><p>Probabilities or entropies can be estimated from <code>Dataset</code>s.</p><ul><li><code>probabilities(x::AbstractDataset, est::NaiveKernel)</code>. Associates a probability <code>p</code> to    each point in <code>x</code>.</li><li><code>genentropy(x::AbstractDataset, est::NaiveKernel)</code>.  Associate probability <code>p</code> to each    point in <code>x</code>, then compute the generalized entropy from those probabilities.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">using Entropy, DelayEmbeddings
pts = Dataset([rand(5) for i = 1:10000]);
œµ = 0.2
est_direct = NaiveKernel(œµ, DirectDistance())
est_tree = NaiveKernel(œµ, TreeDistance())

p_direct = probabilities(pts, est_direct)
p_tree = probabilities(pts, est_tree)

# Check that both methods give the same probabilities
all(p_direct .== p_tree)</code></pre><p>See also: <a href="#Entropies.DirectDistance"><code>DirectDistance</code></a>, <a href="#Entropies.TreeDistance"><code>TreeDistance</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.TreeDistance" href="#Entropies.TreeDistance"><code>Entropies.TreeDistance</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TreeDistance(metric::M = Euclidean()) &lt;: KernelEstimationMethod</code></pre><p>Pairwise distances are evaluated using a tree-based approach with the provided <code>metric</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.DirectDistance" href="#Entropies.DirectDistance"><code>Entropies.DirectDistance</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DirectDistance(metric::M = Euclidean()) &lt;: KernelEstimationMethod</code></pre><p>Pairwise distances are evaluated directly using the provided <code>metric</code>.</p></div></section></article><h3 id="Nearest-neighbor-based"><a class="docs-heading-anchor" href="#Nearest-neighbor-based">Nearest neighbor based</a><a id="Nearest-neighbor-based-1"></a><a class="docs-heading-anchor-permalink" href="#Nearest-neighbor-based" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.KozachenkoLeonenko" href="#Entropies.KozachenkoLeonenko"><code>Entropies.KozachenkoLeonenko</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><p><strong>Nearest neighbour(NN) based</strong></p><pre><code class="language-none">KozachenkoLeonenko(; w::Int = 0) &lt;: NearestNeighborEntropyEstimator</code></pre><p>Entropy estimator based on nearest neighbors. This implementation is based on Kozachenko  &amp; Leonenko (1987)<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzy≈Ñska and Gambin (2016)<sup class="footnote-reference"><a id="citeref-Charzy≈Ñska2016" href="#footnote-Charzy≈Ñska2016">[Charzy≈Ñska2016]</a></sup>.</p><p><code>w</code> is the Theiler window (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>This estimator is only available for entropy estimation. Probabilities  cannot be obtained directly.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Kraskov" href="#Entropies.Kraskov"><code>Entropies.Kraskov</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><p><strong>k-th nearest neighbour(kNN) based</strong></p><pre><code class="language-none">Kraskov(k::Int = 1, w::Int = 1) &lt;: NearestNeighborEntropyEstimator</code></pre><p>Entropy estimator based on <code>k</code>-th nearest neighbor searches<sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>.</p><p><code>w</code> is the number of nearest neighbors to exclude when searching for neighbours  (defaults to <code>0</code>, meaning that only the point itself is excluded).</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>This estimator is only available for entropy estimation. Probabilities  cannot be obtained directly.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="TransferEntropy.Kraskov1" href="#TransferEntropy.Kraskov1"><code>TransferEntropy.Kraskov1</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Kraskov1(k::Int = 1; metric_x = Chebyshev(), metric_y = Chebyshev()) &lt;: MutualInformationEstimator</code></pre><p>The <span>$I^{(1)}$</span> nearest neighbor based mutual information estimator from  Kraskov et al. (2004), using <code>k</code> nearest neighbors. The distance metric for  the marginals <span>$x$</span> and <span>$y$</span> can be chosen separately, while the <code>Chebyshev</code> metric  is always used for the <code>z = (x, y)</code> joint space.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/TransferEntropy.jl/blob/d54cfcd3158c03631f138669c2b217f47b0b7328/src/mutualinfo/nearestneighbor.jl#L15-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TransferEntropy.Kraskov2" href="#TransferEntropy.Kraskov2"><code>TransferEntropy.Kraskov2</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Kraskov2(k::Int = 1; metric_x = Chebyshev(), metric_y = Chebyshev()) &lt;: MutualInformationEstimator</code></pre><p>The <span>$I^{(2)}(x, y)$</span> nearest neighbor based mutual information estimator from  Kraskov et al. (2004), using <code>k</code> nearest neighbors. The distance metric for  the marginals <span>$x$</span> and <span>$y$</span> can be chosen separately, while the <code>Chebyshev</code> metric  is always used for the <code>z = (x, y)</code> joint space.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/TransferEntropy.jl/blob/d54cfcd3158c03631f138669c2b217f47b0b7328/src/mutualinfo/nearestneighbor.jl#L34-L41">source</a></section></article><h2 id="Transfer-entropy"><a class="docs-heading-anchor" href="#Transfer-entropy">Transfer entropy</a><a id="Transfer-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="TransferEntropy.transferentropy" href="#TransferEntropy.transferentropy"><code>TransferEntropy.transferentropy</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><p><strong>General interface</strong></p><pre><code class="language-none">transferentropy(s, t, [c], est; base = 2, Œ± = 1, 
    œÑT = -1, œÑS = -1, Œ∑ùíØ = 1, dT = 1, dS = 1, dùíØ = 1, [œÑC = -1, dC = 1])</code></pre><p>Estimate transfer entropy from source <code>s</code> to target <code>t</code>, potentially conditioned on <code>c</code> (<span>$TE(s \to t$</span>/<span>$TE(s \to t | c)$</span>),with logarithms to the given <code>base</code>, using the  provided entropy/probability estimator <code>est</code>.</p><p>The input series <code>s</code>, <code>t</code>, and <code>c</code> are equal-length real-valued vectors of length <code>N</code>.</p><p>The relation between the embedding lags <code>œÑT</code>, <code>œÑS</code>, <code>œÑC</code>, the <code>Œ∑ùíØ</code> (prediction lag), and  the embedding dimensions <code>dT</code>, <code>dS</code>, <code>dC</code>, <code>dùíØ</code> is given below. </p><p><strong>Nearest neighbor based</strong></p><pre><code class="language-none">transferentropy(s, t, [c], est::Kraskov; base = 2, ...)
transferentropy(s, t, [c], est::KozachenkoLeonenko; base = 2, ...)</code></pre><p>Estimate <span>$TE(s \to t$</span>/<span>$TE(s \to t | c)$</span> using naive nearest neighbor estimators.</p><p>For these estimators, only Shannon entropy can be computed (so the keyword <code>Œ±</code> does not  work). </p><p>See also <a href="#Entropies.Kraskov"><code>Kraskov</code></a>, <a href="@ref"><code>KozacheckoLeonenko</code></a>.</p><p><strong>Kernel density based</strong></p><pre><code class="language-none">transferentropy(s, t, [c], est::NaiveKernel{Union{TreeDistance, DirectDistance}}; 
    base = 2, Œ± = 1,  ...)</code></pre><p>Estimate <span>$TE(s \to t$</span>/<span>$TE(s \to t | c)$</span> using naive kernel density estimation of  probabilities.</p><p>See also <a href="#Entropies.NaiveKernel"><code>NaiveKernel</code></a>, <a href="#Entropies.TreeDistance"><code>TreeDistance</code></a>, <a href="#Entropies.DirectDistance"><code>DirectDistance</code></a>.</p><p><strong>Instantenous Hilbert amplitudes/phases</strong></p><pre><code class="language-none">transferentropy(s, t, [c], est::Hilbert; base = 2, Œ± = 1,  ...)</code></pre><p>Estimate <span>$TE(s \to t$</span>/<span>$TE(s \to t | c)$</span> by first applying the Hilbert transform  to <code>s</code>, <code>t</code> (<code>c</code>) and then estimating transfer entropy.</p><p>See also [<code>Hilbert</code>], <a href="@ref"><code>Amplitude</code></a>, <a href="@ref"><code>Phase</code></a>.</p><p><strong>Symbolic/permutation</strong></p><pre><code class="language-none">transferentropy(s, t, [c], est::SymbolicPermutation; 
    base = 2, Œ± = 1, m::Int = 3, œÑ::Int = 1, ...)
transferentropy!(symb_s, symb_t, s, t, [c], est::SymbolicPermutation; 
    base = 2, Œ± = 1, m::Int = 3, œÑ::Int = 1, ...)</code></pre><p>Estimate <span>$TE(s \to t$</span>/<span>$TE(s \to t | c)$</span> using permutation entropies. This is done  by first symbolizing the input series <code>s</code> and <code>t</code> (both of length <code>N</code>) using motifs of  size <code>m</code> and a time delay of <code>œÑ</code>. The series of motifs are encoded as integer symbol time  series preserving the permutation information. These symbol time series are embedded as  usual, and transfer entropy is computed from marginal entropies of that generalized embedding.</p><p>Optionally, provide pre-allocated (integer) symbol vectors <code>symb_s</code> and <code>symb_t</code>, where <code>length(symb_s) == length(symb_t) == N - (est.m-1)*est.œÑ</code>. This is useful for saving  memory allocations for repeated computations.</p><p>See also <a href="@ref"><code>SymbolicPermutation</code></a>.</p><p><strong>Details</strong></p><p>Transfer entropy (TE) between two simultaneously measured time series,  <span>$s(n) = \{ s_1, s_2, \ldots, s_N \}$</span> and <span>$t(n) = \{ s_1, s_2, \ldots, s_N \}$</span>, is defined as </p><p class="math-container">\[TE(s \to t) = \sum_i p(s_n, t_n, t_{n+\eta}) \log \left( \dfrac{p(t_{n+\eta} |¬†t_n, s_n)}{p(t_{n+\eta} |¬†t_n)} \right)\]</p><p>Above, the TE is expressed in in simplest, scalar form. Including more than one  historical/future value can be done by defining the generalized delay embeddings</p><ul><li><span>$\mathcal{T}^{(d_{\mathcal T}, \eta_{\mathcal T})} = \{t_i^{(d_{\mathcal T}, \eta_{\mathcal T})} \}_{i=1}^{N}$</span></li><li><span>$T^{d_T, \tau_T} = \{t_i^{(d_T, \tau_T)} \}_{i=1}^{N}$</span></li><li><span>$S^{d_S, \tau_S} = \{s_i^{(d_T, \tau_T)} \}_{i=1}^{N}$</span>, </li><li><span>$C^{d_C, \tau_C} = \{s_i^{(d_C, \tau_C)} \}_{i=1}^{N}$</span>, </li></ul><p>each having <code>N</code> distinct states, where the  <span>$d_T$</span>-dimensional, <span>$d_S$</span>-dimensional and <span>$d_C$</span>-dimensional state vectors  comprising <span>$T$</span>, <span>$S$</span> and <span>$C$</span> are constructed with embedding lags  <span>$\tau_T$</span>, <span>$\tau_S$</span>, and <span>$\tau_C$</span>, respectively. The <span>$d_{\mathcal T}$</span>-dimensional  future states <span>$\mathcal{T}^{(d_{\mathcal T}, \eta_{\mathcal T})}$</span> are constructed with prediction lag <span>$\eta_{\mathcal T}$</span> (i.e. predictions go from  present/past states to future states spanning a maximum of  <span>$d_{\mathcal T} \eta_{\mathcal T}$</span> time steps ).</p><p><em>Note: in the original transfer entropy paper, only the historical states are defined as  potentially higher-dimensional, while the future states are always scalar.</em></p><p>The non-conditioned and conditioned generalized forms of the transfer entropy is then</p><p class="math-container">\[TE(s \to t) = \sum_i p(S,T, \mathcal{T}) \log \left( \dfrac{p(\mathcal{T} |¬†T, S)}{p(\mathcal{T} |¬†T)} \right)\]</p><p class="math-container">\[TE(s \to t |¬†c) = \sum_i p(S,T, \mathcal{T}, C) \log \left( \dfrac{p(\mathcal{T} |¬†T, S, C)}{p(\mathcal{T} |¬†T, C)} \right)\]</p><p><strong>Estimation</strong></p><p>Transfer entropy is here estimated by rewriting the generalized transfer entropy, using  properties of logarithms and conditional probabilities, as a sum of marginal entropies</p><p class="math-container">\[TE(s \to t) = H(\mathcal T, T) + H(\mathcal T, S) - H(T) - H(\mathcal T, T, S),\]</p><p class="math-container">\[TE(s \to t | c) = H(\mathcal T, T, C) + H(\mathcal T, S, C) - H(T, C) - H(\mathcal T, T, S, C),\]</p><p>where <span>$H(\cdot)$</span> is the generalized Renyi entropy. Individual marginal entropies are  here computed using the provided estimator <code>est</code>. In the original transfer entropy  paper, the Shannon entropy is used. Here, by adjusting the keyword <code>Œ±</code> (defaults to <code>Œ±=1</code>  for Shannon entropy), the transfer entropy, using the generalized Renyi enropy of order <code>Œ±</code>, can be computed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/TransferEntropy.jl/blob/d54cfcd3158c03631f138669c2b217f47b0b7328/src/transferentropy/interface.jl#L36-L159">source</a></section></article><h2 id="Mutual-information"><a class="docs-heading-anchor" href="#Mutual-information">Mutual information</a><a id="Mutual-information-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-information" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="TransferEntropy.mutualinfo" href="#TransferEntropy.mutualinfo"><code>TransferEntropy.mutualinfo</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><p><strong>General interface</strong></p><pre><code class="language-none">mutualinfo(x, y, est; base = 2, Œ± = 1)</code></pre><p>Estimate mutual information (<span>$I$</span>) between <code>x</code> and <code>y</code> using the provided  entropy/probability estimator <code>est</code>, with logarithms to the given <code>base</code>. Optionally, use  the generalized R√©nyi entropy of order <code>Œ±</code> (defaults to <code>Œ± = 1</code>, which is the Shannon  entropy). See details below.</p><p>Both <code>x</code> and <code>y</code> can be vectors or (potentially multivariate) <a href="#DelayEmbeddings.Dataset"><code>Dataset</code></a>s.</p><p><strong>Binning based</strong></p><pre><code class="language-none">mutualinfo(x, y, est::VisitationFrequency{RectangularBinning}; base = 2, Œ± = 1)</code></pre><p>Estimate <span>$I(x, y)$</span> using a visitation frequency estimator. </p><p>See also <a href="#Entropies.VisitationFrequency"><code>VisitationFrequency</code></a>, <a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a>.</p><p><strong>Kernel density based</strong></p><pre><code class="language-none">mutualinfo(x, y, est::NaiveKernel{Union{DirectDistance, TreeDistance}}; base = 2, Œ± = 1)</code></pre><p>Estimate <span>$I(x, y)$</span> using a naive kernel density estimator. </p><p>It is possible to use both direct evaluation of distances, and a tree-based approach.  Which approach is faster depends on the application. </p><p>See also <a href="#Entropies.NaiveKernel"><code>NaiveKernel</code></a>, <a href="#Entropies.DirectDistance"><code>DirectDistance</code></a>, <a href="#Entropies.TreeDistance"><code>TreeDistance</code></a>.</p><p><strong>Nearest neighbor based</strong></p><pre><code class="language-none">mutualinfo(x, y, est::KozachenkoLeonenko; base = 2)
mutualinfo(x, y, est::Kraskov; base = 2)
mutualinfo(x, y, est::Kraskov1; base = 2)
mutualinfo(x, y, est::Kraskov2; base = 2)</code></pre><p>Estimate <span>$I(x, y)$</span> using a nearest neighbor based estimator. Choose between naive  estimation using the <a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a> or <a href="#Entropies.Kraskov"><code>Kraskov</code></a> entropy estimators,  or the improved <a href="#TransferEntropy.Kraskov1"><code>Kraskov1</code></a> and <a href="#TransferEntropy.Kraskov2"><code>Kraskov2</code></a> dedicated <span>$I$</span> estimators. The  latter estimators reduce bias compared to the naive estimators.</p><p><em>Note: only Shannon entropy is possible to use for these estimators</em>. </p><p>See also <a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#Entropies.Kraskov"><code>Kraskov</code></a>, <a href="#TransferEntropy.Kraskov1"><code>Kraskov1</code></a>,  <a href="#TransferEntropy.Kraskov2"><code>Kraskov2</code></a>.</p><p><strong>Details/estimation</strong></p><p>Mutual information is defined as </p><p class="math-container">\[I(X; Y) = \sum_{y \in Y} \sum_{x \in X} p(x, y) \log \left( \dfrac{p(x, y)}{p(x)p(y)} \right)\]</p><p>This expression can be expressed as the sum of marginal entropies as follows:</p><p class="math-container">\[I(X; Y) = H(X) + H(Y) - H(X, Y).\]</p><p>These individual entropies are computed using the provided entropy/probabilities estimator. For some estimators, it is possible to use generalized order-<code>Œ±</code> R√©nyi entropies for the  <span>$I(x, y)$</span> computation, but the default is to use the Shannon entropy (<code>Œ± = 1</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/TransferEntropy.jl/blob/d54cfcd3158c03631f138669c2b217f47b0b7328/src/mutualinfo/interface.jl#L5-L70">source</a></section></article><h2 id="Dataset"><a class="docs-heading-anchor" href="#Dataset">Dataset</a><a id="Dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Dataset" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DelayEmbeddings.Dataset" href="#DelayEmbeddings.Dataset"><code>DelayEmbeddings.Dataset</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Dataset{D, T} &lt;: AbstractDataset{D,T}</code></pre><p>A dedicated interface for datasets. It contains <em>equally-sized datapoints</em> of length <code>D</code>, represented by <code>SVector{D, T}</code>. These data are contained in the field <code>.data</code> of a dataset, as a standard Julia <code>Vector{SVector}</code>.</p><p>When indexed with 1 index, a <code>dataset</code> is like a vector of datapoints. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables.</p><p><code>Dataset</code> also supports most sensible operations like <code>append!, push!, hcat, eachrow</code>, among others.</p><p><strong>Description of indexing</strong></p><p>In the following let <code>i, j</code> be integers,  <code>typeof(data) &lt;: AbstractDataset</code> and <code>v1, v2</code> be <code>&lt;: AbstractVector{Int}</code> (<code>v1, v2</code> could also be ranges).</p><ul><li><code>data[i]</code> gives the <code>i</code>th datapoint (returns an <code>SVector</code>)</li><li><code>data[v1]</code> will return a vector of datapoints</li><li><code>data[v1, :]</code> using a <code>Colon</code> as a second index will return a <code>Dataset</code> of these points</li><li><code>data[:, j]</code> gives the <code>j</code>th variable timeseries, as <code>Vector</code></li><li><code>data[v1, v2]</code> returns a <code>Dataset</code> with the appropriate entries (first indices being &quot;time&quot;/point index, while second being variables)</li><li><code>data[i, j]</code> value of the <code>j</code>th variable, at the <code>i</code>th timepoint</li></ul><p>Use <code>Matrix(dataset)</code> or <code>Dataset(matrix)</code> to convert. It is assumed that each <em>column</em> of the <code>matrix</code> is one variable. If you have various timeseries vectors <code>x, y, z, ...</code> pass them like <code>Dataset(x, y, z, ...)</code>. You can use <code>columns(dataset)</code> to obtain the reverse, i.e. all columns of the dataset in a tuple.</p></div></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-PrichardTheiler1995"><a class="tag is-link" href="#citeref-PrichardTheiler1995">PrichardTheiler1995</a>Prichard, D., &amp; Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.</li><li class="footnote" id="footnote-Charzy≈Ñska2016"><a class="tag is-link" href="#citeref-Charzy≈Ñska2016">Charzy≈Ñska2016</a>Charzy≈Ñska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., St√∂gbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li></ul></section></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 11 December 2020 15:20">Friday 11 December 2020</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
