var documenterSearchIndex = {"docs":
[{"location":"estimators/kraskov/#Kraskov-estimator","page":"Kraskov estimator","title":"Kraskov estimator","text":"","category":"section"},{"location":"estimators/kraskov/","page":"Kraskov estimator","title":"Kraskov estimator","text":"The Kraskov estimator decomposes the transfer entropy into a sum of mutual informations, which are estimated using the I2 algorithm from Kraskov et al. [1].","category":"page"},{"location":"estimators/kraskov/","page":"Kraskov estimator","title":"Kraskov estimator","text":"using StateSpaceReconstruction\nusing TransferEntropy","category":"page"},{"location":"estimators/kraskov/#From-an-array-of-points-(each-column-being-a-point)","page":"Kraskov estimator","title":"From an array of points (each column being a point)","text":"","category":"section"},{"location":"estimators/kraskov/","page":"Kraskov estimator","title":"Kraskov estimator","text":"pts = rand(3, 100)\nk = 3\n\ntarget_future = [1]\ntarget_presentpast = [2]\nsource_presentpast = [3]\nconditioned_presentpast = Int[]\n\nv = TEVars(target_future,\n            target_presentpast,\n            source_presentpast,\n            conditioned_presentpast)\n","category":"page"},{"location":"estimators/kraskov/#References","page":"Kraskov estimator","title":"References","text":"","category":"section"},{"location":"estimators/kraskov/","page":"Kraskov estimator","title":"Kraskov estimator","text":"Kraskov, Alexander, Harald St√∂gbauer, and Peter Grassberger. \"Estimating","category":"page"},{"location":"estimators/kraskov/","page":"Kraskov estimator","title":"Kraskov estimator","text":"mutual information.\" Physical review E 69.6 (2004): 066138.","category":"page"},{"location":"#TransferEntropy.jl","page":"TransferEntropy.jl","title":"TransferEntropy.jl","text":"","category":"section"},{"location":"","page":"TransferEntropy.jl","title":"TransferEntropy.jl","text":"This package exports two functions, transferentropy and mutualinfo. Both functions use the estimators listed below (not all estimators are implemented for both  functions; see docstrings for transferentropy and mutualinfo for  details).","category":"page"},{"location":"#Estimators","page":"TransferEntropy.jl","title":"Estimators","text":"","category":"section"},{"location":"#Binning-based","page":"TransferEntropy.jl","title":"Binning based","text":"","category":"section"},{"location":"","page":"TransferEntropy.jl","title":"TransferEntropy.jl","text":"VisitationFrequency\nRectangularBinning","category":"page"},{"location":"#Entropies.VisitationFrequency","page":"TransferEntropy.jl","title":"Entropies.VisitationFrequency","text":"VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by the binning scheme r.\n\nExample\n\n# Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\nb = RectangularBinning(5)\n\n# A probabilities estimator that, when applied a dataset, computes visitation frequencies\n# over the boxes of the binning, constructed as describedon the previous line.\nest = VisitationFrequency(b)\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"type"},{"location":"#Entropies.RectangularBinning","page":"TransferEntropy.jl","title":"Entropies.RectangularBinning","text":"RectangularBinning(œµ) <: RectangularBinningScheme\n\nInstructions for creating a rectangular box partition using the binning scheme œµ.  Binning instructions are deduced from the type of œµ.\n\nRectangular binnings may be automatically adjusted to the data in which the RectangularBinning  is applied, as follows:\n\nœµ::Int divides each coordinate axis into œµ equal-length intervals,   extending the upper bound 1/100th of a bin size to ensure all points are covered.\nœµ::Float64 divides each coordinate axis into intervals of fixed size œµ, starting   from the axis minima until the data is completely covered by boxes.\nœµ::Vector{Int} divides the i-th coordinate axis into œµ[i] equal-length   intervals, extending the upper bound 1/100th of a bin size to ensure all points are   covered.\nœµ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size œµ[i], starting   from the axis minima until the data is completely covered by boxes.\n\nRectangular binnings may also be specified on arbitrary min-max ranges. \n\nœµ::Tuple{Vector{Tuple{Float64,Float64}},Int64} creates intervals   along each coordinate axis from ranges indicated by a vector of (min, max) tuples, then divides   each coordinate axis into an integer number of equal-length intervals. Note: this does not ensure   that all points are covered by the data (points outside the binning are ignored).\n\nExample 1: Grid deduced automatically from data (partition guaranteed to cover data points)\n\nFlexible box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then  split each of those data ranges (with some tiny padding on the edges) into 10 equal-length  intervals. This gives (hyper-)rectangular boxes, and works for data of any dimension.\n\nusing Entropies\nRectangularBinning(10)\n\nNow, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then  splits the range along the first coordinate axis (with some tiny padding on the edges)  into 10 equal-length intervals, and the range along the second coordinate axis (with some  tiny padding on the edges) into 5 equal-length intervals. This gives (hyper-)rectangular boxes.\n\nusing Entropies\nRectangularBinning([10, 5])\n\nFixed box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis,  then split the axis ranges into equal-length intervals of fixed size 0.5 until the all data  points are covered by boxes. This approach yields (hyper-)cubic boxes, and works for  data of any dimension.\n\nusing Entropies\nRectangularBinning(0.5)\n\nAgain, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis into equal-length intervals of size 0.3, and the range along the second axis into equal-length intervals of size 0.1 (in both cases,  making sure the data are completely covered by the boxes). This approach gives a (hyper-)rectangular boxes. \n\nusing Entropies\nRectangularBinning([0.3, 0.1])\n\nExample 2: Custom grids (partition not guaranteed to cover data points):\n\nAssume the data consists of 3-dimensional points (x, y, z), and that we want a grid  that is fixed over the intervals [x‚ÇÅ, x‚ÇÇ] for the first dimension, over [y‚ÇÅ, y‚ÇÇ] for the second dimension, and over [z‚ÇÅ, z‚ÇÇ] for the third dimension. We when want to split each of those ranges into 4 equal-length pieces. Beware: some points may fall  outside the partition if the intervals are not chosen properly (these points are  simply discarded). \n\nThe following binning specification produces the desired (hyper-)rectangular boxes. \n\nusing Entropies, DelayEmbeddings\n\nD = Dataset(rand(100, 3));\n\nx‚ÇÅ, x‚ÇÇ = 0.5, 1 # not completely covering the data, which are on [0, 1]\ny‚ÇÅ, y‚ÇÇ = -2, 1.5 # covering the data, which are on [0, 1]\nz‚ÇÅ, z‚ÇÇ = 0, 0.5 # not completely covering the data, which are on [0, 1]\n\nœµ = [(x‚ÇÅ, x‚ÇÇ), (y‚ÇÅ, y‚ÇÇ), (z‚ÇÅ, z‚ÇÇ)], 4 # [interval 1, interval 2, ...], n_subdivisions\n\nRectangularBinning(œµ)\n\n\n\n\n\n","category":"type"},{"location":"#Kernel-density-based","page":"TransferEntropy.jl","title":"Kernel density based","text":"","category":"section"},{"location":"","page":"TransferEntropy.jl","title":"TransferEntropy.jl","text":"NaiveKernel\nTreeDistance\nDirectDistance","category":"page"},{"location":"#Entropies.NaiveKernel","page":"TransferEntropy.jl","title":"Entropies.NaiveKernel","text":"NaiveKernel(œµ::Real, method::KernelEstimationMethod = TreeDistance()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as  discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by  counting how many other points occupy the space spanned by  a hypersphere of radius œµ around mathbfx, according to:\n\nP_i( mathbfx epsilon) approx dfrac1N sum_s neq i  Kleft( dfracmathbfx_i - mathbfx_s epsilon right)\n\nwhere K(z) = 1 if z  1 and zero otherwise. Probabilities are then normalized.\n\nMethods\n\nTree-based evaluation of distances using TreeDistance. Faster, but more   memory allocation.\nDirect evaluation of distances using DirectDistance. Slower, but less    memory allocation. Also works for complex numbers.\n\nEstimation\n\nProbabilities or entropies can be estimated from Datasets.\n\nprobabilities(x::AbstractDataset, est::NaiveKernel). Associates a probability p to    each point in x.\ngenentropy(x::AbstractDataset, est::NaiveKernel).  Associate probability p to each    point in x, then compute the generalized entropy from those probabilities.\n\nExamples\n\nusing Entropy, DelayEmbeddings\npts = Dataset([rand(5) for i = 1:10000]);\nœµ = 0.2\nest_direct = NaiveKernel(œµ, DirectDistance())\nest_tree = NaiveKernel(œµ, TreeDistance())\n\np_direct = probabilities(pts, est_direct)\np_tree = probabilities(pts, est_tree)\n\n# Check that both methods give the same probabilities\nall(p_direct .== p_tree)\n\nSee also: DirectDistance, TreeDistance.\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"#Entropies.TreeDistance","page":"TransferEntropy.jl","title":"Entropies.TreeDistance","text":"TreeDistance(metric::M = Euclidean()) <: KernelEstimationMethod\n\nPairwise distances are evaluated using a tree-based approach with the provided metric.\n\n\n\n\n\n","category":"type"},{"location":"#Entropies.DirectDistance","page":"TransferEntropy.jl","title":"Entropies.DirectDistance","text":"DirectDistance(metric::M = Euclidean()) <: KernelEstimationMethod\n\nPairwise distances are evaluated directly using the provided metric.\n\n\n\n\n\n","category":"type"},{"location":"#Nearest-neighbor-based","page":"TransferEntropy.jl","title":"Nearest neighbor based","text":"","category":"section"},{"location":"","page":"TransferEntropy.jl","title":"TransferEntropy.jl","text":"KozachenkoLeonenko\nKraskov\nKraskov1\nKraskov2","category":"page"},{"location":"#Entropies.KozachenkoLeonenko","page":"TransferEntropy.jl","title":"Entropies.KozachenkoLeonenko","text":"Nearest neighbour(NN) based\n\nKozachenkoLeonenko(; w::Int = 0) <: NearestNeighborEntropyEstimator\n\nEntropy estimator based on nearest neighbors. This implementation is based on Kozachenko  & Leonenko (1987)[KozachenkoLeonenko1987], as described in Charzy≈Ñska and Gambin (2016)[Charzy≈Ñska2016].\n\nw is the Theiler window (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\ninfo: Info\nThis estimator is only available for entropy estimation. Probabilities  cannot be obtained directly.\n\n[Charzy≈Ñska2016]: Charzy≈Ñska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"#Entropies.Kraskov","page":"TransferEntropy.jl","title":"Entropies.Kraskov","text":"k-th nearest neighbour(kNN) based\n\nKraskov(k::Int = 1, w::Int = 1) <: NearestNeighborEntropyEstimator\n\nEntropy estimator based on k-th nearest neighbor searches[Kraskov2004].\n\nw is the number of nearest neighbors to exclude when searching for neighbours  (defaults to 0, meaning that only the point itself is excluded).\n\ninfo: Info\nThis estimator is only available for entropy estimation. Probabilities  cannot be obtained directly.\n\n[Kraskov2004]: Kraskov, A., St√∂gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"#TransferEntropy.Kraskov1","page":"TransferEntropy.jl","title":"TransferEntropy.Kraskov1","text":"Kraskov1(k::Int = 1; metric_x = Chebyshev(), metric_y = Chebyshev()) <: MutualInformationEstimator\n\nThe I^(1) nearest neighbor based mutual information estimator from  Kraskov et al. (2004), using k nearest neighbors. The distance metric for  the marginals x and y can be chosen separately, while the Chebyshev metric  is always used for the z = (x, y) joint space.\n\n\n\n\n\n","category":"type"},{"location":"#TransferEntropy.Kraskov2","page":"TransferEntropy.jl","title":"TransferEntropy.Kraskov2","text":"Kraskov2(k::Int = 1; metric_x = Chebyshev(), metric_y = Chebyshev()) <: MutualInformationEstimator\n\nThe I^(2)(x y) nearest neighbor based mutual information estimator from  Kraskov et al. (2004), using k nearest neighbors. The distance metric for  the marginals x and y can be chosen separately, while the Chebyshev metric  is always used for the z = (x, y) joint space.\n\n\n\n\n\n","category":"type"},{"location":"#Transfer-entropy","page":"TransferEntropy.jl","title":"Transfer entropy","text":"","category":"section"},{"location":"","page":"TransferEntropy.jl","title":"TransferEntropy.jl","text":"transferentropy","category":"page"},{"location":"#TransferEntropy.transferentropy","page":"TransferEntropy.jl","title":"TransferEntropy.transferentropy","text":"General interface\n\ntransferentropy(s, t, [c], est; base = 2, Œ± = 1, \n    œÑT = -1, œÑS = -1, Œ∑ùíØ = 1, dT = 1, dS = 1, dùíØ = 1, [œÑC = -1, dC = 1])\n\nEstimate transfer entropy from source s to target t, potentially conditioned on c (TE(s to t/TE(s to t  c)),with logarithms to the given base, using the  provided entropy/probability estimator est.\n\nThe input series s, t, and c are equal-length real-valued vectors of length N.\n\nThe relation between the embedding lags œÑT, œÑS, œÑC, the Œ∑ùíØ (prediction lag), and  the embedding dimensions dT, dS, dC, dùíØ is given below. \n\nNearest neighbor based\n\ntransferentropy(s, t, [c], est::Kraskov; base = 2, ...)\ntransferentropy(s, t, [c], est::KozachenkoLeonenko; base = 2, ...)\n\nEstimate TE(s to t/TE(s to t  c) using naive nearest neighbor estimators.\n\nFor these estimators, only Shannon entropy can be computed (so the keyword Œ± does not  work). \n\nSee also Kraskov, KozacheckoLeonenko.\n\nKernel density based\n\ntransferentropy(s, t, [c], est::NaiveKernel{Union{TreeDistance, DirectDistance}}; \n    base = 2, Œ± = 1,  ...)\n\nEstimate TE(s to t/TE(s to t  c) using naive kernel density estimation of  probabilities.\n\nSee also NaiveKernel, TreeDistance, DirectDistance.\n\nInstantenous Hilbert amplitudes/phases\n\ntransferentropy(s, t, [c], est::Hilbert; base = 2, Œ± = 1,  ...)\n\nEstimate TE(s to t/TE(s to t  c) by first applying the Hilbert transform  to s, t (c) and then estimating transfer entropy.\n\nSee also [Hilbert], Amplitude, Phase.\n\nSymbolic/permutation\n\ntransferentropy(s, t, [c], est::SymbolicPermutation; \n    base = 2, Œ± = 1, m::Int = 3, œÑ::Int = 1, ...)\ntransferentropy!(symb_s, symb_t, s, t, [c], est::SymbolicPermutation; \n    base = 2, Œ± = 1, m::Int = 3, œÑ::Int = 1, ...)\n\nEstimate TE(s to t/TE(s to t  c) using permutation entropies. This is done  by first symbolizing the input series s and t (both of length N) using motifs of  size m and a time delay of œÑ. The series of motifs are encoded as integer symbol time  series preserving the permutation information. These symbol time series are embedded as  usual, and transfer entropy is computed from marginal entropies of that generalized embedding.\n\nOptionally, provide pre-allocated (integer) symbol vectors symb_s and symb_t, where length(symb_s) == length(symb_t) == N - (est.m-1)*est.œÑ. This is useful for saving  memory allocations for repeated computations.\n\nSee also SymbolicPermutation.\n\nDetails\n\nTransfer entropy (TE) between two simultaneously measured time series,  s(n) =  s_1 s_2 ldots s_N  and t(n) =  s_1 s_2 ldots s_N , is defined as \n\nTE(s to t) = sum_i p(s_n t_n t_n+eta) log left( dfracp(t_n+eta ¬†t_n s_n)p(t_n+eta ¬†t_n) right)\n\nAbove, the TE is expressed in in simplest, scalar form. Including more than one  historical/future value can be done by defining the generalized delay embeddings\n\nmathcalT^(d_mathcal T eta_mathcal T) = t_i^(d_mathcal T eta_mathcal T) _i=1^N\nT^d_T tau_T = t_i^(d_T tau_T) _i=1^N\nS^d_S tau_S = s_i^(d_T tau_T) _i=1^N, \nC^d_C tau_C = s_i^(d_C tau_C) _i=1^N, \n\neach having N distinct states, where the  d_T-dimensional, d_S-dimensional and d_C-dimensional state vectors  comprising T, S and C are constructed with embedding lags  tau_T, tau_S, and tau_C, respectively. The d_mathcal T-dimensional  future states mathcalT^(d_mathcal T eta_mathcal T) are constructed with prediction lag eta_mathcal T (i.e. predictions go from  present/past states to future states spanning a maximum of  d_mathcal T eta_mathcal T time steps ).\n\nNote: in the original transfer entropy paper, only the historical states are defined as  potentially higher-dimensional, while the future states are always scalar.\n\nThe non-conditioned and conditioned generalized forms of the transfer entropy is then\n\nTE(s to t) = sum_i p(ST mathcalT) log left( dfracp(mathcalT ¬†T S)p(mathcalT ¬†T) right)\n\nTE(s to t ¬†c) = sum_i p(ST mathcalT C) log left( dfracp(mathcalT ¬†T S C)p(mathcalT ¬†T C) right)\n\nEstimation\n\nTransfer entropy is here estimated by rewriting the generalized transfer entropy, using  properties of logarithms and conditional probabilities, as a sum of marginal entropies\n\nTE(s to t) = H(mathcal T T) + H(mathcal T S) - H(T) - H(mathcal T T S)\n\nTE(s to t  c) = H(mathcal T T C) + H(mathcal T S C) - H(T C) - H(mathcal T T S C)\n\nwhere H(cdot) is the generalized Renyi entropy. Individual marginal entropies are  here computed using the provided estimator est. In the original transfer entropy  paper, the Shannon entropy is used. Here, by adjusting the keyword Œ± (defaults to Œ±=1  for Shannon entropy), the transfer entropy, using the generalized Renyi enropy of order Œ±, can be computed.\n\n\n\n\n\n","category":"function"},{"location":"#Mutual-information","page":"TransferEntropy.jl","title":"Mutual information","text":"","category":"section"},{"location":"","page":"TransferEntropy.jl","title":"TransferEntropy.jl","text":"mutualinfo","category":"page"},{"location":"#TransferEntropy.mutualinfo","page":"TransferEntropy.jl","title":"TransferEntropy.mutualinfo","text":"General interface\n\nmutualinfo(x, y, est; base = 2, Œ± = 1)\n\nEstimate mutual information (I) between x and y using the provided  entropy/probability estimator est, with logarithms to the given base. Optionally, use  the generalized R√©nyi entropy of order Œ± (defaults to Œ± = 1, which is the Shannon  entropy). See details below.\n\nBoth x and y can be vectors or (potentially multivariate) Datasets.\n\nBinning based\n\nmutualinfo(x, y, est::VisitationFrequency{RectangularBinning}; base = 2, Œ± = 1)\n\nEstimate I(x y) using a visitation frequency estimator. \n\nSee also VisitationFrequency, RectangularBinning.\n\nKernel density based\n\nmutualinfo(x, y, est::NaiveKernel{Union{DirectDistance, TreeDistance}}; base = 2, Œ± = 1)\n\nEstimate I(x y) using a naive kernel density estimator. \n\nIt is possible to use both direct evaluation of distances, and a tree-based approach.  Which approach is faster depends on the application. \n\nSee also NaiveKernel, DirectDistance, TreeDistance.\n\nNearest neighbor based\n\nmutualinfo(x, y, est::KozachenkoLeonenko; base = 2)\nmutualinfo(x, y, est::Kraskov; base = 2)\nmutualinfo(x, y, est::Kraskov1; base = 2)\nmutualinfo(x, y, est::Kraskov2; base = 2)\n\nEstimate I(x y) using a nearest neighbor based estimator. Choose between naive  estimation using the KozachenkoLeonenko or Kraskov entropy estimators,  or the improved Kraskov1 and Kraskov2 dedicated I estimators. The  latter estimators reduce bias compared to the naive estimators.\n\nNote: only Shannon entropy is possible to use for these estimators. \n\nSee also KozachenkoLeonenko, Kraskov, Kraskov1,  Kraskov2.\n\nDetails/estimation\n\nMutual information is defined as \n\nI(X Y) = sum_y in Y sum_x in X p(x y) log left( dfracp(x y)p(x)p(y) right)\n\nThis expression can be expressed as the sum of marginal entropies as follows:\n\nI(X Y) = H(X) + H(Y) - H(X Y)\n\nThese individual entropies are computed using the provided entropy/probabilities estimator. For some estimators, it is possible to use generalized order-Œ± R√©nyi entropies for the  I(x y) computation, but the default is to use the Shannon entropy (Œ± = 1).\n\n\n\n\n\n","category":"function"},{"location":"#Dataset","page":"TransferEntropy.jl","title":"Dataset","text":"","category":"section"},{"location":"","page":"TransferEntropy.jl","title":"TransferEntropy.jl","text":"Dataset","category":"page"},{"location":"#DelayEmbeddings.Dataset","page":"TransferEntropy.jl","title":"DelayEmbeddings.Dataset","text":"Dataset{D, T} <: AbstractDataset{D,T}\n\nA dedicated interface for datasets. It contains equally-sized datapoints of length D, represented by SVector{D, T}. These data are contained in the field .data of a dataset, as a standard Julia Vector{SVector}.\n\nWhen indexed with 1 index, a dataset is like a vector of datapoints. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables.\n\nDataset also supports most sensible operations like append!, push!, hcat, eachrow, among others.\n\nDescription of indexing\n\nIn the following let i, j be integers,  typeof(data) <: AbstractDataset and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges).\n\ndata[i] gives the ith datapoint (returns an SVector)\ndata[v1] will return a vector of datapoints\ndata[v1, :] using a Colon as a second index will return a Dataset of these points\ndata[:, j] gives the jth variable timeseries, as Vector\ndata[v1, v2] returns a Dataset with the appropriate entries (first indices being \"time\"/point index, while second being variables)\ndata[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(dataset) or Dataset(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like Dataset(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"}]
}
