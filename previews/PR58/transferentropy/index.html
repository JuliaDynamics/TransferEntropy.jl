<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Transfer entropy Â· TransferEntropy.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">TransferEntropy.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">TransferEntropy.jl</a></li><li><a class="tocitem" href="../mutualinfo/">Mutual information</a></li><li class="is-active"><a class="tocitem" href>Transfer entropy</a></li><li><a class="tocitem" href="../dataset/">Datasets</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Transfer entropy</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Transfer entropy</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/TransferEntropy.jl/blob/master/docs/src/transferentropy.md" title="Edit on GitHub"><span class="docs-icon fab">ï‚›</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><article class="docstring"><header><a class="docstring-binding" id="TransferEntropy.transferentropy" href="#TransferEntropy.transferentropy"><code>TransferEntropy.transferentropy</code></a> â€” <span class="docstring-category">Function</span></header><section><div><p><strong>Transfer entropy</strong></p><p>Transfer entropy between two simultaneously measured scalar time series <span>$s(n)$</span> and <span>$t(n)$</span>,   <span>$s(n) = \{ s_1, s_2, \ldots, s_N \}$</span> and <span>$t(n) = \{ t_1, t_2, \ldots, t_N \}$</span>, is is defined as </p><p class="math-container">\[TE(s \to t) = \sum_i p(s_i, t_i, t_{i+\eta}) \log \left( \dfrac{p(t_{i+\eta} |Â t_i, s_i)}{p(t_{i+\eta} |Â t_i)} \right)\]</p><p>Including more than one historical/future value can be done by defining the vector-valued time series</p><ul><li><span>$\mathcal{T}^{(d_{\mathcal T}, \eta_{\mathcal T})} = \{t_i^{(d_{\mathcal T}, \eta_{\mathcal T})} \}_{i=1}^{N}$</span></li><li><span>$T^{d_T, \tau_T} = \{t_i^{(d_T, \tau_T)} \}_{i=1}^{N}$</span></li><li><span>$S^{d_S, \tau_S} = \{s_i^{(d_T, \tau_T)} \}_{i=1}^{N}$</span>, </li><li><span>$C^{d_C, \tau_C} = \{s_i^{(d_C, \tau_C)} \}_{i=1}^{N}$</span>, </li></ul><p>each having <code>N</code> distinct states, where the  <span>$d_T$</span>-dimensional, <span>$d_S$</span>-dimensional and <span>$d_C$</span>-dimensional state vectors  comprising <span>$T$</span>, <span>$S$</span> and <span>$C$</span> are constructed with embedding lags  <span>$\tau_T$</span>, <span>$\tau_S$</span>, and <span>$\tau_C$</span>, respectively. The <span>$d_{\mathcal T}$</span>-dimensional  future states <span>$\mathcal{T}^{(d_{\mathcal T}, \eta_{\mathcal T})}$</span> are constructed with prediction lag <span>$\eta_{\mathcal T}$</span> (i.e. predictions go from  present/past states to future states spanning a maximum of  <span>$d_{\mathcal T} \eta_{\mathcal T}$</span> time steps ).</p><p><em>Note: in the original transfer entropy paper, only the historical states are defined as  potentially higher-dimensional, while the future states are always scalar.</em></p><p>The non-conditioned and conditioned generalized forms of the transfer entropy is then</p><p class="math-container">\[TE(s \to t) = \sum_i p(S,T, \mathcal{T}) \log \left( \dfrac{p(\mathcal{T} |Â T, S)}{p(\mathcal{T} |Â T)} \right)\]</p><p class="math-container">\[TE(s \to t |Â c) = \sum_i p(S,T, \mathcal{T}, C) \log \left( \dfrac{p(\mathcal{T} |Â T, S, C)}{p(\mathcal{T} |Â T, C)} \right)\]</p><p><strong>Estimation</strong></p><p>Transfer entropy is here estimated by rewriting the generalized transfer entropy, using  properties of logarithms and conditional probabilities, as a sum of marginal entropies</p><p class="math-container">\[TE(s \to t) = H(\mathcal T, T) + H(\mathcal T, S) - H(T) - H(\mathcal T, T, S),\]</p><p class="math-container">\[TE(s \to t | c) = H(\mathcal T, T, C) + H(\mathcal T, S, C) - H(T, C) - H(\mathcal T, T, S, C),\]</p><p>where <span>$H(\cdot)$</span> is the generalized Renyi entropy. Individual marginal entropies are  here computed using the provided estimator <code>est</code>. In the original transfer entropy  paper, the Shannon entropy is used. Here, by adjusting the keyword <code>Î±</code> (defaults to <code>Î±=1</code>  for Shannon entropy), the transfer entropy, using the generalized Renyi enropy of order <code>Î±</code>, can be computed.</p><p><strong>General interface</strong></p><pre><code class="language-none">transferentropy(s, t, [c], est; base = 2, Î± = 1, 
    Ï„T = -1, Ï„S = -1, Î·ğ’¯ = 1, dT = 1, dS = 1, dğ’¯ = 1, [Ï„C = -1, dC = 1])</code></pre><p>Estimate transfer entropy from source <code>s</code> to target <code>t</code> (<span>$TE(s \to t)$</span>), using the  provided entropy/probability estimator <code>est</code> and RÃ©nyi entropy of order-<code>Î±</code>, with  logarithms to the given <code>base</code>. Optionally, condition on <code>c</code> (<span>$TE(s \to t | c)$</span>). </p><p>The relation between the embedding lags <code>Ï„T</code>, <code>Ï„S</code>, <code>Ï„C</code>, the <code>Î·ğ’¯</code> (prediction lag), and  the embedding dimensions <code>dT</code>, <code>dS</code>, <code>dC</code>, <code>dğ’¯</code> is given above.</p><p>The input series <code>s</code>, <code>t</code>, and <code>c</code> are equal-length real-valued vectors of length <code>N</code>.</p><p><strong>Nearest neighbor based</strong></p><pre><code class="language-none">transferentropy(s, t, [c], est::Kraskov; base = 2, ...)
transferentropy(s, t, [c], est::KozachenkoLeonenko; base = 2, ...)</code></pre><p>Estimate <span>$TE(s \to t)$</span>/<span>$TE(s \to t | c)$</span> using naive nearest neighbor estimators.</p><p>For these estimators, only Shannon entropy can be computed (so the keyword <code>Î±</code> does not  work). </p><p>See also <a href="../estimators/#Entropies.Kraskov"><code>Kraskov</code></a>, <a href="@ref"><code>KozacheckoLeonenko</code></a>.</p><p><strong>Kernel density based</strong></p><pre><code class="language-none">transferentropy(s, t, [c], est::NaiveKernel{Union{TreeDistance, DirectDistance}}; 
    base = 2, Î± = 1,  ...)</code></pre><p>Estimate <span>$TE(s \to t)$</span>/<span>$TE(s \to t | c)$</span> using naive kernel density estimation of  probabilities.</p><p>See also <a href="../estimators/#Entropies.NaiveKernel"><code>NaiveKernel</code></a>, <a href="../estimators/#Entropies.TreeDistance"><code>TreeDistance</code></a>, <a href="../estimators/#Entropies.DirectDistance"><code>DirectDistance</code></a>.</p><p><strong>Instantenous Hilbert amplitudes/phases</strong></p><pre><code class="language-none">transferentropy(s, t, [c], est::Hilbert; base = 2, Î± = 1,  ...)</code></pre><p>Estimate <span>$TE(s \to t)$</span>/<span>$TE(s \to t | c)$</span> by first applying the Hilbert transform  to <code>s</code>, <code>t</code> (<code>c</code>) and then estimating transfer entropy.</p><p>See also [<code>Hilbert</code>], <a href="@ref"><code>Amplitude</code></a>, <a href="@ref"><code>Phase</code></a>.</p><p><strong>Symbolic/permutation</strong></p><pre><code class="language-none">transferentropy(s, t, [c], est::SymbolicPermutation; 
    base = 2, Î± = 1, m::Int = 3, Ï„::Int = 1, ...)
transferentropy!(symb_s, symb_t, s, t, [c], est::SymbolicPermutation; 
    base = 2, Î± = 1, m::Int = 3, Ï„::Int = 1, ...)</code></pre><p>Estimate <span>$TE(s \to t)$</span>/<span>$TE(s \to t | c)$</span> using permutation entropies. This is done  by first symbolizing the input series <code>s</code> and <code>t</code> (both of length <code>N</code>) using motifs of  size <code>m</code> and a time delay of <code>Ï„</code>. The series of motifs are encoded as integer symbol time  series preserving the permutation information. These symbol time series are embedded as  usual, and transfer entropy is computed from marginal entropies of that generalized embedding.</p><p>Optionally, provide pre-allocated (integer) symbol vectors <code>symb_s</code> and <code>symb_t</code>, where <code>length(symb_s) == length(symb_t) == N - (est.m-1)*est.Ï„</code>. This is useful for saving  memory allocations for repeated computations.</p><p>See also <a href="../estimators/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/TransferEntropy.jl/blob/087912ae7dbd8e781c30e824c7ca898d156d2340/src/transferentropy/interface.jl#L36-L159">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mutualinfo/">Â« Mutual information</a><a class="docs-footer-nextpage" href="../dataset/">Datasets Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 11 December 2020 22:36">Friday 11 December 2020</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
